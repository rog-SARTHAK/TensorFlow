{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOptedhkeDCMNAgE+WWwwab"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Link: https://www.youtube.com/watch?v=x_VrgWTKkiM&list=PLQY2H8rRoyvwWuPiWnuTDBHe7I0fMSsfO&index=3"],"metadata":{"id":"6rd3sgtIVAFo"}},{"cell_type":"markdown","source":["Fashion mnist has only one object/subject in image which is centered; which is a limitation. Also it has to be the only subject in image. A numeric label represented that subject. It is not trained to identify multiple subjects in the image. For this reason we'll use Convolutional NN. "],"metadata":{"id":"hDmUaiAIScw1"}},{"cell_type":"markdown","source":["In Convolutional NN, the idea is that we filter the images before training the deep NN. After filtering the images, features within the images could then come to the forefront and then you would then spot those features to identify something."],"metadata":{"id":"YCzmSBvXUCz-"}},{"cell_type":"markdown","source":["A filter is simply a set of multipliers. Image is a matrix of pixels. We will multiply CURRENT_PIXEL_VALUE by FILTER DEFINITION (which is also a matrix). Summed up and we get new pixel value."],"metadata":{"id":"WulxuijwWEG3"}},{"cell_type":"markdown","source":["For example a filter when multiplied by contents of an image, it removes everything except vertical lines."],"metadata":{"id":"JYerh6-bYZGn"}},{"cell_type":"markdown","source":["This Filtering can be combined with Pooling, which groups up the pixels in the image and filters them down to a subset."],"metadata":{"id":"EQINgR5bYwr8"}},{"cell_type":"markdown","source":["For example , max pooling two by two will group the image into sets of 2*2 pixels and simply pick the largest. The image will be reduced to a quarter of its original size but the features can still be maintained."],"metadata":{"id":"hs_YjHHdZG0C"}},{"cell_type":"markdown","source":["Features are actually learned in CNN. They are just parameters like those in the neurons of a neural network."],"metadata":{"id":"CdbhvEt0a-G7"}},{"cell_type":"markdown","source":["When our image is fed into the convolutional layer, a number of randomly initialized filters will pass over the image. The result of these are fed into next layer and matching is performed by the NN. And over time, the filters that give us the image outputs that give the best matches will be learned and the process is called Feature Extraction."],"metadata":{"id":"Y1dzw0QePuin"}},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    \n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu', \n","                           input_shape=(28,28,1)),        #this layer takes i/p\n","                          #so we specify the i/p shape, and it will generate\n","                          #64 filters and multiply each of them across the image.\n","                          #then epoch per epoch, it will figure out which filters\n","                          #give the best signals to help match the images to their\n","                          #labels in much the same way it learned which parameters\n","                          #worked best in the dense layer.\n","\n","    tf.keras.layers.MaxPooling2D(2,2), #this max pooling layer compress the image\n","                          #and enhance the features\n","\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n","])"],"metadata":{"id":"yozUst0-Wpzp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have a flattened input that's fed into a dense layer that in turn is fed into the final dense layer that is our O/P. Only difference is the input shape is NOT SPECIFIED. That's because we'll put a convolutional layer on top of it."],"metadata":{"id":"CioDyUx4YOWG"}},{"cell_type":"code","source":["model = tf.keras.models.Sequential([\n","    \n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu',\n","                           input_shape=(28, 28, 1)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])"],"metadata":{"id":"m8wnHZrCdiCJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can stack Convolutional layers on top of each other to really breakdown the image and try to learn from very abstract features. The network thus starts to learn based on the features of the image instead of just the raw pattern of pixels."],"metadata":{"id":"5fdLi4UCgLYw"}}]}